{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf400
{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;\f1\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;\cssrgb\c0\c0\c0;}
\vieww16400\viewh11380\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs36 \cf2 From: "Lisa L. Harlow" <em@editorialmanager.com>\
Subject: Your Submission MET-2017-0979R1 - [EMID:a13e9f4ae676a558]\
Date: June 1, 2018 at 3:35:45 AM PDT\
To: "Jeffrey N Rouder" <rouderj@missouri.edu>\
Reply-To: "Lisa L. Harlow" <lharlow@uri.edu>\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\fs24 \
\pard\pardeftab720\sl420\partightenfactor0

\fs36 \cf3 \expnd0\expndtw0\kerning0
CC: pmethods@nd.edu, hao.wu.5@bc.edu, lharlow@uri.edu\
\
MET-2017-0979R1\
Beyond Overall Effects: A Bayesian Approach to Finding Constraints Across A Collection Of Studies In Meta-Analysis\
\pard\pardeftab720\sl420\partightenfactor0

\i \cf3 Psychological Methods
\i0 \
\
Dear Dr. Rouder:\
\
I invited the same three reviewers to evaluate your revision. The third reviewer was not able to take on this assignment. Both the first two reviewers recommend major revision. Judging from their comments, they are not completely convinced by your reply to some of their points and would like to see some more efforts in revision. In particular, Reviewer 1 hopes to see the traditional meta-analysis method applied to the examples and contrasted with the new method; Reviewer 2 hopes to see additional decision methods such as credible interval, DIC and ppp illustrated and contrasted with the BF in the examples. I believe, to help the general audience of this journal understand your innovation and to maximize the impact of your contribution, these are reasonable demands.\
\
I myself am impressed by your efforts in addressing my comments through the additional section Alternative Models and simulations. But I would hope you clarify on the following issues in the next revision.\
\
1. In the last paragraph of page 24, your discussion suggests that equivalence testing is a product out of a weakness of frequentist testing (that the null cannot be accepted) and is therefore not an option in Bayesian analysis. I would not agree with this position. In my previous decision letter, I introduced this line of thought not from the perspective of statistical equivalent testing, but from the current trend in psychology to focus on effect size. In this trend only an effect size large enough to be practically meaningful signals a "finding" and a significant but trivial effect size is not considered interesting. Would you believe the current shift of interest to a non-trivial rather than non-null effect size is also an artifact produced to get around the weakness of frequentist tests? Especially you did not cite any research to back your position.\
\
2. In the simulation study shown in Figure 9, you explained the failure of the current setting to detect an exponentially or half-normally distributed set of positive true effects by noting that the prior distribution assumes the positive true effects cannot be highly concentrated around zero. This would need some clarification. Although a truncated normal distribution as specified on page 9 does predict positive effects to concentrate somewhere away from zero, when we take into account of the prior distribution of the \\mu_\\theta parameter (Figure 2, left panel), this does not seem correct. The marginal distribution of any single effect (after marginalizing the prior on \\mu_\\theta) is just a half normal, which has a concentration around zero.\
\
By the way, the caption of Figure 9 contains a typo in the last line: the BF is not unconstrained model vs null model, but vs positive model judging from the discussion on pages 25-26. Please also mention that the models being compared are those on page 9, not any of the true data generating models.\
\
If you decide to revise the work, please submit a list of changes or a rebuttal against each point which is being raised when you submit the revised manuscript.\
\
If you have not already done so as part of your Author Note, please provide the details (2-4 sentences) of prior dissemination of the ideas and data appearing in the manuscript (e.g., if some or all of the data and ideas in the manuscript were presented at a conference or meeting posted on a listserv, shared on a website, etc.).\
\
We request that your revision arrive by 06/21/2018. If for some reason you find that you cannot meet this deadline, please contact us as soon as possible in order to make other arrangements.\
\
To submit a revision, go to https://met.editorialmanager.com/ and log in as an Author. You will see a menu item called Submission Needing Revision. You will find your submission record there.\
\
Sincerely,\
Hao Wu, Ph.D.\
Associate Editor\

\i Psychological Methods
\i0 \
\
\
################\
Reviewers' comments\
################\
\
Reviewer #1:\
\
While I see some advantages of the approach now, I am still missing a convincing argument why this way of performing a meta-analysis should be performed. Maybe one could do the following but probably the authors have better ideas: 1) Translate the traditional way to do meta-analysis into a model. 2) Show that this model is not always chosen as the best model in the examples. 3) Show how the conclusions would differ between using the traditional model and the best model.\
\
Some minor issues:\
\
- Table Captions seems to be missing\
- Figure 3 seems to be mentioned before Figure 1\
- Figure 2 column 1 row 2 does not seem to match with the common model. The figure seems to imply that the effects may not be negative, which is not mentioned in the text.\
- Figure 4 description exceeds page margins\
\
Github supplement:\
\
I still was not able to compile the document. After installing the needed libraries, I got the following error:\
\
Quitting from lines 216-226 (p.Rmd)\
Error in nWayAOV(dat$Y, X, gMap, rscale = c(rScaleIndv, rScaleIndv, rScaleMean), :\
Length of rscale vector wrong. Was 7 and should be 3.\
\
I urge the authors, to check whether the paper can be compiled on at least two computers (with different operating systems) that were not used for writing the paper.\
\
In addition, I would like to suggest the following changes to make reproducing easier:\
\
- Give the paper Rmd file a better name than p.Rmd\
- Automate the installation of all libraries (see for example, https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them)\
\
###########################\
\
\
Reviewer #2:\
\
This authors well addressed most of my previous questions, but I still have two concerns.\
\
First, I still don't understand why we cannot check the posteriors of \\theta_i in the unconstrained model to determine whether they are 0, equal, positive or can be negative. Then, only one model needs to be fitted. In the response letter, the authors stated that "rules about posteriors, say that the 95% CI includes zero, neither fall from Bayes rule nor yield well-calibrated inference". However, 95% CIs are summary quantities of the posterior distributions. If the CI doesn't cover a certain value, say 0, it means that this certain value is not a credible value for this parameter. To applied researchers, the 95% CI is more informative because it provides the range of the parameter values so the applied researchers are able to tell whether the effect is relatively large or small, rather than just positive or negative. I wonder whether the authors can compare this to the model comparison approach. If no difference is found, then using 95% CIs are practically preferred. If there is a difference, the paper will be more persuasive to recommend the model comparison approach.\
\
Second, the authors used Bayes factors for model comparison, but Bayes factors are sensitive to priors. In the author's own paper (Rouder et al., 2009), it says that "arbitrarily diffuse priors are not appropriate for hypothesis testing". In fact, noninformative priors provide Bayes factors that almost always support the null hypothesis. The authors stated that "it is best to avoid judgments that Bayes factor model comparisons depend too little or too much on priors. They depend on it to the degree they do. Whatever this degree, it is the degree resulting from the usage of Bayes rule, which in turn mandates that evidence for competing positions are the degree to which they improve predictive accuracy." However, even when Bayes rule is followed, people can still make wrong statistical conclusions. In this paper, the authors only picked informative priors. But in practice, it is very common that informative priors are not available, so researchers have to or just prefer to use noninformative priors. If this is the case, researchers will make wrong decisions based on Bayes factor. DIC and posterior predictive p-value may work better. Both of them are easier to obtain than Bayes factors because they are based on posteriors. I would suggest the authors to include the model comparison results based on DIC and ppp value as well. If the results are similar to Bayes factor, applied researchers may prefer the other two criteria as they are easier to obtain, possibly less sensitive to priors, and they do not emphasize on binary decisions.}